---
title: "ANOVA"
author: "Danielle A Presgraves"
date: "9/21/2018"
output: word_document
---
### Introduction: 
ANOVA, like a lot of the statistics that we use, is fundamentally about trying to differentiate between any actual 'signal' in the data from the intrinsic 'noise' that is due to sampling. It attempts to answer the question: Does the treatment explain any differences in mean between treatment groups or is it just due to sampling particular individuals? If the answer is "it is just due to the particular make up of individuals in our data set" than we will see different results when we sample different individuals. Comparing the variance between treatment and sample is explicitly declared in it's name: AN(alysis).O(f).VA(riance); we are literally analysing the variance and seeing if we can attribute it to either a treatment effect or if it is just noise and using this attribution to support if the means between our groups are different.  

Remember that, as we have discussed in Chapter 6,7 and 8, every test has certain assumptions that must be met in order to justify (or believe) the results of that test. For One-way ANOVA, the important assumptions are about the normality of the data points in each group and the equality of variances in each group. It might be obvious to you that, since ANOVA is based on partitioning out the variance into the 'noise' and the 'meaningful signal', violating the assumption of equal variance is devastating for one way ANOVA. This means that we will construct our usual pipeline (stating the null and alternate hypotheses, naming the test and and investigate any assumptions) before conducting the test. There is a non-parametric version of oneway ANOVA called Kruskall-Wallis that we can use if we find that our variances are not equal. 

Additionally, if we are able to reject the null hypothesis that the means of our groups are the same, we can then follow up our ANOVA with a post-hoc test called TukeyHSD. This test compares each pair of groups and determines which one (or more than one) has a different mean than the other groups. There is a non-parametric version of this post-hoc test, too, but it is in a different package that I don't want to force you to download. So we will wave our hands a bit and use Tukey HSD as the post-hoc test even when we have been forced to substitute a  Kruskall-Wallis test for an ANOVA. 

For our example, investigating any relationship between the amount of Maize produced in a region and the level of malaria present (the vector that spreads malaria uses maize as a food source), we use the following general terminology: 

DV = Dependent Variable; it is the response variable

Factor = what we are testing, ie. different treatments, medications etc.

### Malaria and Maize production.
(from the R book, chapter 9) The pollen of the corn (maize) plant is known to be a source of food to larval mosquitoes of the species Anopheles arabiensis, the main vector of malaria in Ethiopia. In 2010, malaria caused an estimated 660 000 deaths (with an uncertainty range of 490 000 to 836 000), mostly among African children. The production of maize has increased substantially in certain areas of Ethiopia recently, and over the same time malaria has entered into new areas where it was previously rare. This raises the question: is the increase of maize cultivation partly responsible for the increase in malaria?

In this case, the (alleged) dependent variable is Malaria incidence and Maize is the independent variable and, additionally, has three factor levels: low, medium and high production.

```{r}
# let's read in the data
mal_maize<-read.csv("/Users/daniellepresgraves/Desktop/Bio214/BIOL300 data sets/malariaMaize.csv")
attach(mal_maize)
names(mal_maize)
```
Previously, we used the t-test to demonstrate a test that was fairly robust to violations of assumptions. ANOVA, as a contrast, is ** very sensitive ** to any violations of a couple of assumptions: normally distributed data in each of the factors (low, medium and high categories should have similar normal shapes in their data) and homoscedasticity (the variances should be similar between the low, medium and high categories). This shouldn't surprise you because ANOVA is analyzing the VARIANCE to tell us something about any difference between means of groups so you should expect that it may be particularly sensitive to the assumption that the variances in each category are the same and that the categories are all normally distributed. 

First, let's see if the data set is normally distributed by checking the normal distribution *at each level (low, medium and high cultivation)*:
```{r}
plot(IncidenceRate.10000~Maize_yield,data=mal_maize, 
     main="Cultivation of Maize and rate of Malaria")
# hmmmm... looks like there is an outlier in the low cultivation area.
# Let's look with qqnorm plot since that it often where we begin with this analysis
qqnorm(IncidenceRate.10000)
qqline(IncidenceRate.10000)
# let's apply some of the tests we saw in chapters 6,7 and 8
# normal distributed incidence rate across all the data - this is not
# a requirement for ANOVA which needs to have a normal distribution in each of the categories but not the overall data. We'll use it here so that you can be refreshed on how the test works. 
shapiro.test(IncidenceRate.10000)
#############################
# WHAT IS a requirement for one way ANOVA is that the data in EACH 
# category is normally distributed. Here is how you can test for that: 
# use the by() function which is a 'wrapper' for the tapply function and give it the shapiro.test function as an argument:
cat(" Here is the by function: \n")
# you can also use the print function but the output is subtley different as the two functions do slightly different things
print(" ******* ")
cat("-------------------------- \n")
by(IncidenceRate.10000,Maize_yield, shapiro.test)
# compare the by() wrapper to just using the tapply(): 
cat(" Here is the tapply function: \n")
cat("-------------------------- \n")
tapply(IncidenceRate.10000,Maize_yield, shapiro.test)
# you will notice that the by() function prints out the results to the screen in a more attractive format but there shouldn't be any significant differences since tapply is the basis of the by() function. 
#############################
```
Now let's investigate the REALLY important assumption that each factor (or category) has approximately equal variances. The null hypothesis of this test is: 

$H_o: \sigma_1^2 =\sigma_2^2 = \sigma_3^2$

If we are able to significantly reject this null hypothesis, we will need to use the typical pipeline of transforming the data to see if there is a transformation that will cause the variances to be equal. When that doesn't work (since it will only rarely work), we will adopt the non-parametric version of ANOVA called Kruskal-Wallis. 

So, we'll start with Bartlett: 
```{r}
# is the variance of the incidence rate in each factor level approximately the same?
# Bartlett test of Homogeneity of variances
bartlett.test(IncidenceRate.10000~Maize_yield,data=mal_maize)
# the p-values of those two tests suggest that the assumptions of an
# anova test will not be met!
```
These do not look normally distributed since the p-value is very small so we reject the null hypothesis of variance equality! That's too bad!

Let's move on and determine another aspect of variance that needs to be met to use ANOVA. We need to avoid a 'wedge shape' in our variance versus mean graph. If we can't reject the null hypothesis of homogeneity of variances, the following condition is already met but it is always good to double check! If only, to ensure that nothing about our analysis is questioned by the audience.

In our current case, we have rejected the null hypothesis of variance equality. We can now ask: are the factors homoscedastic? Homoscedastic means 'equal scatter' and it is usually a term associated with ANOVA/correlation/regression. However, it is also useful for identifying the problematic "wedge or funnel shape" of data. One method to test this is to draw a scatterplot of mean versus variance. It may not be intuitive why this is a useful graph but *there are a number of statistical tests that allow for violations in homogeneity of variance but only if the variance does not increase with increasing sample mean (which is the dreaded wedge shape that I have mentioned repeatedly) * so only if the data is approximately homoscedastic. So even if the samples variances are not equal (heteroscedastic), if the variances does not increase with increasing mean than, depending on the test, that still might be okay. We’ll use our old buddy tapply to demonstrate. Of course, there are only three data points in this case so it probably won't be informative for our purposes but it is, in general, an illustrative way of visualizing whether or not the variance increases with increasing mean:  
```{r}
plot(tapply(IncidenceRate.10000,Maize_yield,mean),
     tapply(IncidenceRate.10000,Maize_yield,var), col="Red", 
     main = "Does the variance increase with increasing mean?")
```
Drat!! 

If we our data doesn't behave (ie. it doesn't follow the assumptions), can we force our data to be normally distributed and homeoscedastic by using one of the common transformations? If so, we could then use a parametric test (aov) on th transformed data. If not, we will need to use the non-parametric version of anova called Kruskall-Wallis. Non-parametric versions of tests are less desirable than the parametric version because you give up power when using a non-parametric version and you really don't want to have to give up power in your test. However, sometimes you have to! 

So let's see about a transformating our data first. Let's begin with sqrt+0.5 transformation: 
```{r}
qqnorm(sqrt(IncidenceRate.10000+0.5))
qqline(sqrt(IncidenceRate.10000+0.5))
boxplot(sqrt(IncidenceRate.10000+0.5)~Maize_yield, 
        main="Maize_yield and Malaria Incidence")
# probably still not sufficiently normally distributed to justify using
# it but let's have a look
bartlett.test(sqrt(IncidenceRate.10000+0.5)~Maize_yield,data=mal_maize)
```
It may be starting to behave. ..but not quite enough. Let's try a log transformation since that is pretty common in biology. 

log transformation: 
```{r}
qqnorm(log(IncidenceRate.10000))
qqline(log(IncidenceRate.10000))
boxplot(log(IncidenceRate.10000)~Maize_yield,
        main="TRANSFORMED Maize_yield and Malaria Incidence")
# might want to try to erase ambiguity by using a measurement of 
# normality like shapiro-wilks
shapiro.test(log(IncidenceRate.10000))
# this let's us believe that we can't reject the data sets being
# normally distributed so we should be able to use bartlett test now: 
bartlett.test(log(IncidenceRate.10000)~Maize_yield)
#Nope - no homogeneity of variance!
```

Okay! Now we have data that looks normal-ish enough....although it still rejects equality of variances between the groups so one way ANOVA is probably not appropriate ** (Note: we are working through this example to compare it to the non-parametric alternative so we wouldn't usually continue to use regular ANOVA - aov() - at this point in the 'real world' except that we are using it here specifically to compare it to the non-parametric outcome so we will continue to use it).** Maybe? Does it? That is the artistry of statistics; data is messy and sometimes you continue onwards with an analysis with the potential flaws of your assumptions in the back of your mind so that you may confess them to your audience in your conclusions and your discussion sections.

#### ANOVA:

For a one-way ANOVA, you have various function choices but the most straight-forward option is >aov(DV∼Factor, data=name of file). This function is a ‘wrap’ function of lm(). lm() refers to “Linear Model” function. In fact, it is often more useful to use lm() rather than aov() for one-way ANOVA as long as your design has balanced numbers of individuals. In the case of our data set, which has required transformation, we can obtain an ANOVA table in the following manner:
```{r}
maizeMal_log.aov<-aov(log(IncidenceRate.10000)~Maize_yield)
# let's call the variable where we have put the results of our aov function
maizeMal_log.aov
# aov has the useful feature of allowing you to call summary()
summary(maizeMal_log.aov)
```

Fine! We know that parametric one-way ANOVA is inappropriate for our data, even transformed, because the variances are not equal between the categories. We conducted the analysis so that we would all know how to conduct ANOVA in R. Plus, we now have the results of the one-way parametric ANOVA that will allow us to compare those results with the nonparametric alternative (Kruskall Wallis). Remember that because variances aren't equal, we *should* be using Kruskal Wallis so this is the appropriate test for this particular data set. You could use kruskal wallis on the original, untransformed data set. I have used kruskal on the log transformed data set so that we could compare the results to those obtained from the parametric aov function: 
```{r}
NP_Mal_log.kwt<-kruskal.test(log(IncidenceRate.10000)~Maize_yield)
NP_Mal_log.kwt
```

Here is the non-parametric Kruskal-Wallis test on the original, untransformed data: 
```{r}
NP_Mal.kwt<-kruskal.test(IncidenceRate.10000~Maize_yield)
NP_Mal.kwt
```
** Let's pretend that we were able to reject our Ho that all categories means are equal.** We would now run the post-hoc test. The function to do that is >TukeyHSD(result.aov) and it works by taking pairwise comparisons of each of the factors (for instance, in the case of the malaria and the maize production, Tukey would compare the malaria deaths between the High-Low yields, Low-Medium yields and High-Medium yields and it would put upper and lower bounds on these treatments). In our case, we didn't reject the null hypothesis of the ANOVA in the first place so it is not surprising that none of the three pairwise comparisons results in a significant p-value. 
```{r}
TukeyHSD(maizeMal_log.aov)
```
Unsurprisingly, since we failed to reject the null hypothesis for our data, none of the three pairwise comparisons yields a significant p-value (all the p values are greater than 0.05). This is actually reassuring since it would have been upsetting news if we ran a TukeyHSD after FTR the null and got at least one mean that was significantly different! That would suggest that our analysis had a major problem!


detach at bottom

```{r, echo=FALSE}
detach(mal_maize)
```

